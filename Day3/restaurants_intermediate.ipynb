{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slightly more advanced notebook that fits the restaurants revenue data sets using RFR with a grid optimal parameters searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/cross_validation.py:42: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/chelsea/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## Similar to Regressors_simple...\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder  \n",
    "import scipy as sp\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\n",
    "from sklearn.ensemble import GradientBoostingRegressor  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "trainData = pd.read_csv('data/train.csv', header=0, parse_dates = [1])\n",
    "testData = pd.read_csv('data/test.csv', header=0, parse_dates = [1])\n",
    "\n",
    "# Replace 'Open Date' by a feature representing the age of the resturant in years\n",
    "# Replace 'Type', 'City' and 'City Group' by integer indicators \n",
    "trainData['Open Date'] = (datetime.now() - trainData['Open Date']).astype('timedelta64[D]') / 365   \n",
    "trainData['Type'] = LabelEncoder().fit_transform(trainData['Type'])\n",
    "trainData['City Group'] = LabelEncoder().fit_transform(trainData['City Group'])\n",
    "trainData['City'] = LabelEncoder().fit_transform(trainData['City'])\n",
    "# Separate the Y array\n",
    "Y_train = trainData['revenue']\n",
    "# Drop the Id and Y variable to create the finale X array to be fitted\n",
    "X_train = trainData.drop(['Id','revenue'], axis=1) \n",
    "\n",
    "\n",
    "# Same for Test data\n",
    "testData['Open Date'] = (datetime.now() - testData['Open Date']).astype('timedelta64[D]') / 365   \n",
    "testData['Type'] = LabelEncoder().fit_transform(testData['Type'])\n",
    "testData['City Group'] = LabelEncoder().fit_transform(testData['City Group'])\n",
    "testData['City'] = LabelEncoder().fit_transform(testData['City'])\n",
    "ids = testData['Id'].values\n",
    "testData = testData.drop(['Id'], axis=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search the parameters space and fit a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the parameters grid to search\n",
    "param_grid = {'n_estimators':[100,1000],\n",
    "'max_depth': [1,2,4],\n",
    "'min_samples_leaf': [1, 3, 5],\n",
    "'max_features': [1.0, 0.3, 0.1]}\n",
    "est = RandomForestRegressor()\n",
    "gs_cv = GridSearchCV(est, param_grid,n_jobs=-1, cv=10).fit(X_train, Y_train)\n",
    "\n",
    "# print best fit parameters\n",
    "gs_cv.best_params_\n",
    "\n",
    "\n",
    "# Creating a RFR with the best fit parameters (entered manually)\n",
    "forest=RandomForestRegressor(max_depth= 4, max_features= 0.1, min_samples_leaf= 3, n_estimators= 100)\n",
    "# Fit the training data\n",
    "forest=forest.fit(X_train,Y_train )\n",
    "# Predict the testing data\n",
    "output = forest.predict(testData)\n",
    "\n",
    "\n",
    "# Write into submission file\n",
    "predictions_file = open(\"interRF.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"Id\",\"Prediction\"])\n",
    "open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search the parameters space and fit a Gradient boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters grid to search, notice the learning_rate parameter\n",
    "param_grid2 = {'n_estimators':[100,1000],\n",
    "'max_depth': [1,2,4],\n",
    "'learning_rate': [0.1,0.01],               \n",
    "'min_samples_leaf': [1, 3, 5],\n",
    "'max_features': [1.0, 0.3, 0.1]}\n",
    "est2 = GradientBoostingRegressor()\n",
    "gs_cv2 = GridSearchCV(est2, param_grid2,n_jobs=-1, cv=10).fit(X_train, Y_train)\n",
    "\n",
    "# print best fit parameters\n",
    "gs_cv2.best_params_\n",
    "\n",
    "\n",
    "\n",
    "# Creating a GBR with the best fit parameters (entered manually)\n",
    "gbr=GradientBoostingRegressor(max_depth= 4, max_features= 0.1, min_samples_leaf= 1, n_estimators= 100,learning_rate=0.01)\n",
    "# Fit the training data\n",
    "gbr=gbr.fit(X_train,Y_train )\n",
    "# Predict the testing data\n",
    "output = gbr.predict(testData)\n",
    "\n",
    "\n",
    "\n",
    "# Write into submission file\n",
    "predictions_file = open(\"interGB.csv\", \"w\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"Id\",\"Prediction\"])\n",
    "open_file_object.writerows(zip(ids, output))\n",
    "predictions_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
